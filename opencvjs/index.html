
<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8" />
  <title>TensorFlow.js OBD Demo</title>

  <!-- Load TensorFlow.js-->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js"></script>
  <!-- Load the coco-ssd model. -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.0.2/dist/coco-ssd.js"></script>
  

</head>

<body>

  <h1>Demo of Three.js</h1>
  <video id="video" style=" position:fixed; top:150px; left:10px;" autoplay muted playsInline width="640" height="360"></video>
  <canvas id="canvasOutput" style=" position:fixed; top:600px; left:10px;" width="640" height="360"></canvas>
  <canvas id="canvas0" style=" position:fixed; top:150px; left:10px;" width="640" height="360"></canvas>
  <canvas id="canvas1" style=" position:fixed; top:600px; left:10px;" width="640" height="360"></canvas>
  <div id="container"></div>

  <script type="module">

    import * as THREE from './src/three/three.module.js';

    var camera, scene, renderer;

    var isUserInteracting = false,
      lon = 0, lat = 0,
      phi = 0, theta = 0,
      distance = 50,
      onPointerDownPointerX = 0,
      onPointerDownPointerY = 0,
      onPointerDownLon = 0,
      onPointerDownLat = 0;

    init();
    animate();

    function init() {

      var container, mesh;

      container = document.getElementById( 'container' );

      camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 1, 1100 );
      camera.target = new THREE.Vector3( 0, 0, 0 );

      scene = new THREE.Scene();

      var geometry = new THREE.SphereBufferGeometry( 500, 60, 40 );
      // invert the geometry on the x-axis so that all of the faces point inward
      geometry.scale( - 1, 1, 1 );

      var video = document.getElementById( 'video' );
      //video.play();

      var texture = new THREE.VideoTexture( video );
      var material = new THREE.MeshBasicMaterial( { map: texture } );

      mesh = new THREE.Mesh( geometry, material );

      scene.add( mesh );

      renderer = new THREE.WebGLRenderer();
      renderer.setPixelRatio( window.devicePixelRatio );
      renderer.setSize( window.innerWidth, window.innerHeight );
      container.appendChild( renderer.domElement );

      document.addEventListener( 'mousedown', onDocumentMouseDown, false );
      document.addEventListener( 'mousemove', onDocumentMouseMove, false );
      document.addEventListener( 'mouseup', onDocumentMouseUp, false );
      document.addEventListener( 'wheel', onDocumentMouseWheel, false );

      //

      window.addEventListener( 'resize', onWindowResize, false );

    }

    function onWindowResize() {

      camera.aspect = window.innerWidth / window.innerHeight;
      camera.updateProjectionMatrix();

      renderer.setSize( window.innerWidth, window.innerHeight );

    }

    function onDocumentMouseDown( event ) {

      event.preventDefault();

      isUserInteracting = true;

      onPointerDownPointerX = event.clientX;
      onPointerDownPointerY = event.clientY;

      onPointerDownLon = lon;
      onPointerDownLat = lat;

    }

    function onDocumentMouseMove( event ) {

      if ( isUserInteracting === true ) {

        lon = ( onPointerDownPointerX - event.clientX ) * 0.1 + onPointerDownLon;
        lat = ( event.clientY - onPointerDownPointerY ) * 0.1 + onPointerDownLat;

      }

    }

    function onDocumentMouseUp() {

      isUserInteracting = false;

    }

    function onDocumentMouseWheel( event ) {

      distance += event.deltaY * 0.05;

      distance = THREE.Math.clamp( distance, 1, 50 );

    }

    function animate() {

      requestAnimationFrame( animate );
      update();

    }

    function update() {

      lat = Math.max( - 85, Math.min( 85, lat ) );
      phi = THREE.Math.degToRad( 90 - lat );
      theta = THREE.Math.degToRad( lon );

      camera.position.x = distance * Math.sin( phi ) * Math.cos( theta );
      camera.position.y = distance * Math.cos( phi );
      camera.position.z = distance * Math.sin( phi ) * Math.sin( theta );

      camera.lookAt( camera.target );

      renderer.render( scene, camera );

    }

  </script>

  <script type="text/javascript">
    let video = document.getElementById("video");
    let canvasFrame = document.getElementById("canvasOutput"); // canvasFrame is the id of <canvas>
    let context = canvasFrame.getContext("2d");

    const FPS = 8;
    function startVideo(){
      if (navigator.mediaDevices.getUserMedia || navigator.mediaDevices.webkitGetUserMedia){
        // define a Promise that'll be used to load the webcam and read its frames
        const webcamPromise = navigator.mediaDevices
          .getUserMedia({
            video: {
              width: { ideal: 640 },
              height: { ideal: 360 },
              frameRate: { ideal: 30 },
            },
            //video: true,
            audio: false,
          })
          .then(stream => {
            // pass the current frame to the window.stream
            window.stream = stream;
            // pass the stream to the videoRef
            video.srcObject = stream;

            return new Promise(resolve => {
              video.onloadedmetadata = () => {
                resolve();
              };
            });
          }, (error) => {
            console.log("Couldn't start the webcam")
            console.error(error)
          });
      }
    }
    function processVideo() {
      /*let fisheyeK = cv.matFromArray(3, 3, cv.CV_32FC1, 
                                     [1.59064e+03,   0.00000e+00,   1.91950e+03,
                                      0.00000e+00,   1.58753e+03,   1.07950e+03,
                                      0.00000e+00,   0.00000e+00,   1.00000e+00]);
      let fisheyeD = cv.matFromArray(4, 1, cv.CV_32FC1, 
                                     [-1.09917e-01, 4.63744e-02, -3.09370e-02, 7.79134e-03]);
      let r = cv.matFromArray(4, 3, cv.CV_32FC1,
                              [1, 0, 0, 0, 1, 0, 0, 0, 1]);
      let nK = new cv.Mat(3 , 3, cv.CV_32FC1);*/
      let cap = new cv.VideoCapture("videoInput");
      let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
      let dst = new cv.Mat(video.height, video.width, cv.CV_8UC1);
      let rst = new cv.Mat(video.height, video.width, cv.CV_8UC1);
      let begin = Date.now();
      let fx = 496.74244/2;
      let fy = 495.45232/2;
      let cx = 655.44564/2;
      let cy = 390.89756/2;
      let k1 = -0.222852;
      let k2 = 0.028990;
      let p1 = 0.000416;
      let p2 = -0.001841;
      cap.read(src);
      cv.cvtColor(src, dst, cv.COLOR_RGBA2GRAY);

      for(v = 0; v < video.height; v++){
        for(u = 0; u < video.width; u++){
          let x = (u - cx) / fx;
          let y = (v - cy) / fy;
          let r = Math.sqrt(x * x + y * y);
          let x_distorted = x * (1 + k1 * r * r + k2 * r * r * r * r) +
                            2 * p1 * x * y + p2 * (r * r + 2 * x * x);
          let y_distorted = y * (1 + k1 * r * r + k2 * r * r * r * r) +
                            2 * p2 * x * y + p1 * (r * r + 2 * y * y);
          let u_distorted = fx * x_distorted + cx;
          let v_distorted = fy * y_distorted + cy;
          if(u_distorted >= 0 && v_distorted >= 0 &&
             u_distorted < video.width && v_distorted < video.height){
              rst.ucharPtr(v, u)[0] = dst.ucharPtr(v_distorted, u_distorted)[0];
          }
          else{
            rst.ucharPtr(v, u)[0] = 0;
          }
        }
      }

      cv.imshow("canvasOutput", rst);
      src.delete();
      dst.delete();
      rst.delete();

      const videoPromise = document.getElementById('video');
      const canvasPromise = document.getElementById('canvasOutput');
      // 'mobilenet_v1', 'mobilenet_v2', 'lite_mobilenet_v2'
      // Defaults to 'lite_mobilenet_v2' 
      // lite_mobilenet_v2 is fastest in inference speed.
      // mobilenet_v2 has the highest accuracy.
      const modelPromise = cocoSsd.load({ base: 'lite_mobilenet_v2' });
      Promise.all([modelPromise, videoPromise])
          .then(values => {
              detectFrame(values[1], values[0], "canvas0");
          })
          .catch(error => {
              console.error(error);
          });
      Promise.all([modelPromise, canvasPromise])
          .then(values => {
              detectFrame(values[1], values[0], "canvas1");
          })
          .catch(error => {
              console.error(error);
          });
        
      detectFrame = (video, model, canvasElement) => {
          // IMPORTANT: define video size before detection (equal to window size)
          // Usage: detect(Video element, max detections, threshold)
          model.detect(video).then(predictions => {
              // Debug output
              // console.log('Predictions: ', predictions);
              // Draw predictions
              renderPredictions(predictions, canvasElement);
          });
      };

      renderPredictions = (predictions, canvasElement) => {
          //console.log('Canvas_size: ', canvas.width, canvas.height);
          const canvas = document.getElementById(canvasElement);
          const ctx = document.getElementById(canvasElement).getContext("2d");
          ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
          // Font options
          const font = "14px sans-serif";
          ctx.font = font;
          ctx.textBaseline = "top";
          var personDetected = false;
          predictions.forEach(prediction => {
              const _class = prediction.class;
              // Draw person class only
              if (_class == "person") {
                  // Coordination conversion use real video size and window size
                  const x = prediction.bbox[0];
                  const y = prediction.bbox[1];
                  const width = prediction.bbox[2];
                  const height = prediction.bbox[3];
                  // Draw the bounding box.
                  ctx.strokeStyle = "#00FFFF";
                  ctx.lineWidth = 4;
                  ctx.strokeRect(x, y, width, height);
                  // Draw the label background.
                  ctx.fillStyle = "#00FFFF";
                  const textWidth = ctx.measureText(prediction.class).width;
                  const textHeight = parseInt(font, 10); // base 10
                  ctx.fillRect(x, y, textWidth + 4, textHeight + 4);
                  // Draw the text last to ensure it's on top.
                  ctx.fillStyle = "#000000";
                  ctx.fillText(prediction.class, x, y);
                  ctx.fillText(prediction.score.toFixed(2), x, y + height - textHeight);
                  personDetected = true;
              }
          });
      };

      // schedule next one.
      let delay = 1000/FPS - (Date.now() - begin);
      setTimeout(processVideo, delay);
    }
    function onOpenCvReady() {
      cv['onRuntimeInitialized']=()=>{
        console.log("OPENCVJS LOADED!");
        console.log(cv);
        //alert('OpenCV.js is ready.');
        // do all your work here
        // schedule first one.
        //setTimeout(processVideo, 0);
      };
    }
    startVideo();
  </script>

  <!-- Load opencv.js. -->  
  <script src="https://docs.opencv.org/4.5.1/opencv.js" onload="onOpenCvReady();" type="text/javascript"></script>

  <!-- We will put our React component inside this div. -->
  <div id="root"></div>  
</body>