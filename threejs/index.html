
<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8" />
  <title>A Thress.js Demo</title>
</head>

<body>

  <h1>A Thress.js Demo</h1>
  <video id="video" style=" position:fixed; top:150px; left:10px;" autoplay muted playsInline width="640" height="360"></video>
  <canvas id="canvasOutput" style=" position:fixed; top:600px; left:10px;" width="640" height="360"></canvas>
  <!--canvas id="canvas0" style=" position:fixed; top:150px; left:10px;" width="640" height="360"></canvas>
  <canvas id="canvas1" style=" position:fixed; top:600px; left:10px;" width="640" height="360"></canvas-->
  <div id="container"></div>

  <script src="./three/three.js"></script>
  <script src="./three/js/postprocessing/EffectComposer.js"></script>
  <script src="./three/js/postprocessing/RenderPass.js"></script>
  <script src="./three/js/postprocessing/MaskPass.js"></script>
  <script src="./three/js/postprocessing/ShaderPass.js"></script>
  <script src="./three/js/shaders/CopyShader.js"></script>

  <script>
    // Create scene
    var scene = new THREE.Scene();

    // Create renderer
    var renderer = new THREE.WebGLRenderer();
    renderer.setSize( window.innerWidth, window.innerHeight );
		document.body.appendChild( renderer.domElement );

    // Create camera
    camera = new THREE.PerspectiveCamera( 100, window.innerWidth / window.innerHeight, 1, 1000000 );
    camera.position.z = 800;

    // Create effect composer
    composer = new THREE.EffectComposer(renderer);
    composer.addPass(new THREE.RenderPass(scene, camera));

    // Add distortion effect to effect composer
    var effect = new THREE.ShaderPass(getDistortionShaderDefinition());
    composer.addPass(effect);
    effect.renderToScreen = true;

    // Setup distortion effect
    var horizontalFOV = 140;
    var strength = 0.5;
    var cylindricalRatio = 2;
    var height = Math.tan(THREE.Math.degToRad(horizontalFOV) / 2) / camera.aspect;

    camera.fov = Math.atan(height) * 2 * 180 / 3.1415926535;
    camera.updateProjectionMatrix();

    effect.uniforms[ "strength" ].value = strength;
    effect.uniforms[ "height" ].value = height;
    effect.uniforms[ "aspectRatio" ].value = camera.aspect;
    effect.uniforms[ "cylindricalRatio" ].value = cylindricalRatio;

    function getDistortionShaderDefinition()
    {
      return {
        uniforms: {
            "tDiffuse":         { type: "t", value: null },
            "strength":         { type: "f", value: -1 },
            "height":           { type: "f", value: 1 },
            "aspectRatio":      { type: "f", value: 1 },
            "cylindricalRatio": { type: "f", value: 1 }
        },

        vertexShader: [
            "uniform float strength;",          // s: 0 = perspective, 1 = stereographic
            "uniform float height;",            // h: tan(verticalFOVInRadians / 2)
            "uniform float aspectRatio;",       // a: screenWidth / screenHeight
            "uniform float cylindricalRatio;",  // c: cylindrical distortion ratio. 1 = spherical

            "varying vec3 vUV;",                // output to interpolate over screen
            "varying vec2 vUVDot;",             // output to interpolate over screen

            "void main() {",
                "gl_Position = projectionMatrix * (modelViewMatrix * vec4(position, 1.0));",

                "float scaledHeight = strength * height;",
                "float cylAspectRatio = aspectRatio * cylindricalRatio;",
                "float aspectDiagSq = aspectRatio * aspectRatio + 1.0;",
                "float diagSq = scaledHeight * scaledHeight * aspectDiagSq;",
                "vec2 signedUV = (2.0 * uv + vec2(-1.0, -1.0));",

                "float z = 0.5 * sqrt(diagSq + 1.0) + 0.5;",
                "float ny = (z - 1.0) / (cylAspectRatio * cylAspectRatio + 1.0);",

                "vUVDot = sqrt(ny) * vec2(cylAspectRatio, 1.0) * signedUV;",
                "vUV = vec3(0.5, 0.5, 1.0) * z + vec3(-0.5, -0.5, 0.0);",
                "vUV.xy += uv;",
            "}"
        ].join("\n"),

        fragmentShader: [
            "uniform sampler2D tDiffuse;",      // sampler of rendered scene?s render target
            "varying vec3 vUV;",                // interpolated vertex output data
            "varying vec2 vUVDot;",             // interpolated vertex output data

            "void main() {",
                "vec3 uv = dot(vUVDot, vUVDot) * vec3(-0.5, -0.5, -1.0) + vUV;",
                "gl_FragColor = texture2DProj(tDiffuse, uv);",
            "}"
        ].join("\n")

      };
    }
    
    var geometry = new THREE.PlaneGeometry(1280, 720);
    var videoIn = document.getElementById('video');
    var texture = new THREE.VideoTexture(videoIn);
    var material = new THREE.MeshBasicMaterial({map: texture});
    var mesh = new THREE.Mesh(geometry, material);

    var cubeCamera = new THREE.CubeCamera( 1, 3000, 1024);
    cubeCamera.renderTarget.minFilter = THREE.LinearMipMapLinearFilter;
    scene.add(cubeCamera);
    camParent.add(cubeCamera);
 
    var material = new THREE.MeshBasicMaterial( { envMap: cubeCamera.renderTarget } );
    material.side = THREE.DoubleSide;
    sphere = new THREE.Mesh( new THREE.SphereGeometry( 2, 60, 30 ), material );
    scene.add(sphere);

    scene.add(mesh);

		camera.position.z = 200;

		var render = function () {
			requestAnimationFrame( render );

			//cube.rotation.x += 0.01;
			//cube.rotation.y += 0.01;

			renderer.render(scene, camera);
		};

		render();
  </script>

  <!--script type="module">
    import * as THREE from './three/three.module.js';

    var camera, scene, renderer;

    var isUserInteracting = false,
      lon = 0, lat = 0,
      phi = 0, theta = 0,
      distance = 50,
      onPointerDownPointerX = 0,
      onPointerDownPointerY = 0,
      onPointerDownLon = 0,
      onPointerDownLat = 0;

    init();
    animate();

    function init() {

      var container, mesh;

      container = document.getElementById( 'container' );

      camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 1, 1100 );
      camera.target = new THREE.Vector3( 0, 0, 0 );

      scene = new THREE.Scene();

      var geometry = new THREE.SphereBufferGeometry( 500, 60, 40 );
      // invert the geometry on the x-axis so that all of the faces point inward
      geometry.scale( - 1, 1, 1 );

      var video = document.getElementById( 'video' );
      //video.play();

      var texture = new THREE.VideoTexture( video );
      var material = new THREE.MeshBasicMaterial( { map: texture } );

      mesh = new THREE.Mesh( geometry, material );

      scene.add( mesh );

      renderer = new THREE.WebGLRenderer();
      renderer.setPixelRatio( window.devicePixelRatio );
      renderer.setSize( window.innerWidth, window.innerHeight );
      container.appendChild( renderer.domElement );

      document.addEventListener( 'mousedown', onDocumentMouseDown, false );
      document.addEventListener( 'mousemove', onDocumentMouseMove, false );
      document.addEventListener( 'mouseup', onDocumentMouseUp, false );
      document.addEventListener( 'wheel', onDocumentMouseWheel, false );

      //

      window.addEventListener( 'resize', onWindowResize, false );

    }

    function onWindowResize() {

      camera.aspect = window.innerWidth / window.innerHeight;
      camera.updateProjectionMatrix();

      renderer.setSize( window.innerWidth, window.innerHeight );

    }

    function onDocumentMouseDown( event ) {

      event.preventDefault();

      isUserInteracting = true;

      onPointerDownPointerX = event.clientX;
      onPointerDownPointerY = event.clientY;

      onPointerDownLon = lon;
      onPointerDownLat = lat;

    }

    function onDocumentMouseMove( event ) {

      if ( isUserInteracting === true ) {

        lon = ( onPointerDownPointerX - event.clientX ) * 0.1 + onPointerDownLon;
        lat = ( event.clientY - onPointerDownPointerY ) * 0.1 + onPointerDownLat;

      }

    }

    function onDocumentMouseUp() {

      isUserInteracting = false;

    }

    function onDocumentMouseWheel( event ) {

      distance += event.deltaY * 0.05;

      distance = THREE.Math.clamp( distance, 1, 50 );

    }

    function animate() {

      requestAnimationFrame( animate );
      update();

    }

    function update() {

      lat = Math.max( - 85, Math.min( 85, lat ) );
      phi = THREE.Math.degToRad( 90 - lat );
      theta = THREE.Math.degToRad( lon );

      camera.position.x = distance * Math.sin( phi ) * Math.cos( theta );
      camera.position.y = distance * Math.cos( phi );
      camera.position.z = distance * Math.sin( phi ) * Math.sin( theta );

      camera.lookAt( camera.target );

      renderer.render( scene, camera );

    }

  </script-->

  <script type="text/javascript">
    let video = document.getElementById("video");
    let canvasFrame = document.getElementById("canvasOutput"); // canvasFrame is the id of <canvas>
    let context = canvasFrame.getContext("2d");

    const FPS = 20;
    function startVideo(){
      if (navigator.mediaDevices.getUserMedia || navigator.mediaDevices.webkitGetUserMedia){
        // define a Promise that'll be used to load the webcam and read its frames
        const webcamPromise = navigator.mediaDevices
          .getUserMedia({
            video: {
              width: { ideal: 640 },
              height: { ideal: 360 },
              frameRate: { ideal: 30 },
            },
            //video: true,
            audio: false,
          })
          .then(stream => {
            // pass the current frame to the window.stream
            window.stream = stream;
            // pass the stream to the videoRef
            video.srcObject = stream;

            return new Promise(resolve => {
              video.onloadedmetadata = () => {
                resolve();
              };
            });
          }, (error) => {
            console.log("Couldn't start the webcam")
            console.error(error)
          });
      }
    }
    function processVideo() {
      /*let fisheyeK = cv.matFromArray(3, 3, cv.CV_32FC1, 
                                     [1.59064e+03,   0.00000e+00,   1.91950e+03,
                                      0.00000e+00,   1.58753e+03,   1.07950e+03,
                                      0.00000e+00,   0.00000e+00,   1.00000e+00]);
      let fisheyeD = cv.matFromArray(4, 1, cv.CV_32FC1, 
                                     [-1.09917e-01, 4.63744e-02, -3.09370e-02, 7.79134e-03]);
      let r = cv.matFromArray(4, 3, cv.CV_32FC1,
                              [1, 0, 0, 0, 1, 0, 0, 0, 1]);
      let nK = new cv.Mat(3 , 3, cv.CV_32FC1);*/
      let cap = new cv.VideoCapture("videoInput");
      let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
      let dst = new cv.Mat(video.height, video.width, cv.CV_8UC1);
      let rst = new cv.Mat(video.height, video.width, cv.CV_8UC1);
      let begin = Date.now();
      let fx = 496.74244/2;
      let fy = 495.45232/2;
      let cx = 655.44564/2;
      let cy = 390.89756/2;
      let k1 = -0.222852;
      let k2 = 0.028990;
      let p1 = 0.000416;
      let p2 = -0.001841;
      cap.read(src);
      cv.cvtColor(src, dst, cv.COLOR_RGBA2GRAY);

      for(v = 0; v < video.height; v++){
        for(u = 0; u < video.width; u++){
          let x = (u - cx) / fx;
          let y = (v - cy) / fy;
          let r = Math.sqrt(x * x + y * y);
          let x_distorted = x * (1 + k1 * r * r + k2 * r * r * r * r) +
                            2 * p1 * x * y + p2 * (r * r + 2 * x * x);
          let y_distorted = y * (1 + k1 * r * r + k2 * r * r * r * r) +
                            2 * p2 * x * y + p1 * (r * r + 2 * y * y);
          let u_distorted = fx * x_distorted + cx;
          let v_distorted = fy * y_distorted + cy;
          if(u_distorted >= 0 && v_distorted >= 0 &&
             u_distorted < video.width && v_distorted < video.height){
              rst.ucharPtr(v, u)[0] = dst.ucharPtr(v_distorted, u_distorted)[0];
          }
          else{
            rst.ucharPtr(v, u)[0] = 0;
          }
        }
      }

      cv.imshow("canvasOutput", rst);
      src.delete();
      dst.delete();
      rst.delete();

      const videoPromise = document.getElementById('video');
      const canvasPromise = document.getElementById('canvasOutput');
      // 'mobilenet_v1', 'mobilenet_v2', 'lite_mobilenet_v2'
      // Defaults to 'lite_mobilenet_v2' 
      // lite_mobilenet_v2 is fastest in inference speed.
      // mobilenet_v2 has the highest accuracy.
      const modelPromise = cocoSsd.load({ base: 'lite_mobilenet_v2' });
      Promise.all([modelPromise, videoPromise])
          .then(values => {
              detectFrame(values[1], values[0], "canvas0");
          })
          .catch(error => {
              console.error(error);
          });
      Promise.all([modelPromise, canvasPromise])
          .then(values => {
              detectFrame(values[1], values[0], "canvas1");
          })
          .catch(error => {
              console.error(error);
          });
        
      detectFrame = (video, model, canvasElement) => {
          // IMPORTANT: define video size before detection (equal to window size)
          // Usage: detect(Video element, max detections, threshold)
          model.detect(video).then(predictions => {
              // Debug output
              // console.log('Predictions: ', predictions);
              // Draw predictions
              renderPredictions(predictions, canvasElement);
          });
      };

      renderPredictions = (predictions, canvasElement) => {
          //console.log('Canvas_size: ', canvas.width, canvas.height);
          const canvas = document.getElementById(canvasElement);
          const ctx = document.getElementById(canvasElement).getContext("2d");
          ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
          // Font options
          const font = "14px sans-serif";
          ctx.font = font;
          ctx.textBaseline = "top";
          var personDetected = false;
          predictions.forEach(prediction => {
              const _class = prediction.class;
              // Draw person class only
              if (_class == "person") {
                  // Coordination conversion use real video size and window size
                  const x = prediction.bbox[0];
                  const y = prediction.bbox[1];
                  const width = prediction.bbox[2];
                  const height = prediction.bbox[3];
                  // Draw the bounding box.
                  ctx.strokeStyle = "#00FFFF";
                  ctx.lineWidth = 4;
                  ctx.strokeRect(x, y, width, height);
                  // Draw the label background.
                  ctx.fillStyle = "#00FFFF";
                  const textWidth = ctx.measureText(prediction.class).width;
                  const textHeight = parseInt(font, 10); // base 10
                  ctx.fillRect(x, y, textWidth + 4, textHeight + 4);
                  // Draw the text last to ensure it's on top.
                  ctx.fillStyle = "#000000";
                  ctx.fillText(prediction.class, x, y);
                  ctx.fillText(prediction.score.toFixed(2), x, y + height - textHeight);
                  personDetected = true;
              }
          });
      };

      // schedule next one.
      let delay = 1000/FPS - (Date.now() - begin);
      setTimeout(processVideo, delay);
    }
    function onOpenCvReady() {
      cv['onRuntimeInitialized']=()=>{
        console.log("OPENCVJS LOADED!");
        console.log(cv);
        //alert('OpenCV.js is ready.');
        // do all your work here
        // schedule first one.
        setTimeout(processVideo, 0);
      };
    }
    startVideo();
  </script>

  <!-- Load opencv.js. -->  
  <script src="https://docs.opencv.org/4.5.1/opencv.js" onload="onOpenCvReady();" type="text/javascript"></script>
</body>