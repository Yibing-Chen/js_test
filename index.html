
<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8" />
  <title>TensorFlow.js OBD Demo</title>

  <!-- Load TensorFlow.js-->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js"></script>
  <!-- Load the coco-ssd model. -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.0.2/dist/coco-ssd.js"></script>
  
  <!-- Load React. -->
  <!--script src="https://unpkg.com/react@16/umd/react.development.js" crossorigin></script>
  <script src="https://unpkg.com/react-dom@16/umd/react-dom.development.js" crossorigin></script>

  <script src="https://unpkg.com/babel-standalone@6.26.0/babel.min.js"></script-->

</head>

<body>

  <h1>Demo of TensorFlow.js Coco SSD's model object detection</h1>
  <video id="videoInput" style=" position:fixed; top:150px; left:10px;" autoplay muted playsInline width="640" height="360"></video>
  <canvas id="canvasOutput" style=" position:fixed; top:600px; left:10px;" width="640" height="360"></canvas>
  <canvas id="canvas0" style=" position:fixed; top:150px; left:10px;" width="640" height="360"></canvas>
  <canvas id="canvas1" style=" position:fixed; top:600px; left:10px;" width="640" height="360"></canvas>
  <!-- Load our React component. -->
  <!--script src="src/detect.js" type="text/babel"></script-->

  <!--script src="src/undistort.js"></script-->

  <!--script async src="https://docs.opencv.org/4.5.1/opencv.js" onload="onOpenCvReady();" type="text/javascript"></script-->
  <!--script async src="src/opencv.js" onload="onOpenCvReady();" type="text/javascript"></script-->

  <!--script type="text/javascript">
    function onOpenCvReady() {
      if (cv.getBuildInformation) {
        console.log(cv.getBuildInformation());
        onloadCallback();
      } else {
        //wait for opencv.js compilation;
        cv["onRuntimeInitialized"] = () => {
          console.log(cv.getBuildInformation());
          onloadCallback();
        };
      }
    }
    function onloadCallback() {
      let streaming = false;
      //let startAndStop = document.getElementById("startAndStop");
      let canvasOutput = document.getElementById("canvasOutput");
      let canvasContext = canvasOutput.getContext("2d");

      if (!streaming) {
        startCamera(onVideoStarted);
      }

      function onVideoStarted() {
        streaming = true;
        //startAndStop.innerText = "Stop";

        /**Your example code here*/
        let video = document.getElementById("videoInput");
        let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
        let dst = new cv.Mat(video.height, video.width, cv.CV_8UC1);
        let cap = new cv.VideoCapture(video);

        const FPS = 30;
        function processVideo() {
          try {
            if (!streaming) {
              // clean and stop.
              src.delete();
              dst.delete();
              return;
            }
            let begin = Date.now();
            // start processing.
            cap.read(src);
            cv.cvtColor(src, dst, cv.COLOR_RGBA2GRAY);
            cv.imshow("canvasOutput", dst);
            // schedule the next one.
            let delay = 1000 / FPS - (Date.now() - begin);
            setTimeout(processVideo, delay);
          } catch (err) {000000
            console.log(err);00
          }
        }

        // schedule the first one.
        setTimeout(processVideo, 0);
      }

      function onVideoStopped() {
        streaming = false;
        canvasContext.clearRect(
          0,
          0,
          canvasOutput.width,
          canvasOutput.height
        );
        //startAndStop.innerText = "Start";
      }
    }

    //utils
    function startCamera(callback) {
      let video = document.getElementById("videoInput");
      navigator.mediaDevices
        .getUserMedia({ video: true, audio: false })
        .then(function (stream) {
          video.srcObject = stream;
          video.play();
          callback();
        })
        .catch(function (err) {
          console.log("An error occurred! " + err);
        });
    };
  </script-->

  <script type="text/javascript">

    let video = document.getElementById("videoInput");
    let canvasFrame = document.getElementById("canvasOutput"); // canvasFrame is the id of <canvas>
    let context = canvasFrame.getContext("2d");

    const FPS = 8;
    function startVideo(){
      if (navigator.mediaDevices.getUserMedia || navigator.mediaDevices.webkitGetUserMedia){
        // define a Promise that'll be used to load the webcam and read its frames
        const webcamPromise = navigator.mediaDevices
          .getUserMedia({
            video: {
              width: { ideal: 640 },
              height: { ideal: 360 },
              frameRate: { ideal: 30 },
            },
            //video: true,
            audio: false,
          })
          .then(stream => {
            // pass the current frame to the window.stream
            window.stream = stream;
            // pass the stream to the videoRef
            video.srcObject = stream;

            return new Promise(resolve => {
              video.onloadedmetadata = () => {
                resolve();
              };
            });
          }, (error) => {
            console.log("Couldn't start the webcam")
            console.error(error)
          });
      }
    }
    function processVideo() {
      /*let fisheyeK = cv.matFromArray(3, 3, cv.CV_32FC1, 
                                     [1.59064e+03,   0.00000e+00,   1.91950e+03,
                                      0.00000e+00,   1.58753e+03,   1.07950e+03,
                                      0.00000e+00,   0.00000e+00,   1.00000e+00]);
      let fisheyeD = cv.matFromArray(4, 1, cv.CV_32FC1, 
                                     [-1.09917e-01, 4.63744e-02, -3.09370e-02, 7.79134e-03]);
      let r = cv.matFromArray(4, 3, cv.CV_32FC1,
                              [1, 0, 0, 0, 1, 0, 0, 0, 1]);
      let nK = new cv.Mat(3 , 3, cv.CV_32FC1);*/
      let cap = new cv.VideoCapture("videoInput");
      let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
      let dst = new cv.Mat(video.height, video.width, cv.CV_8UC1);
      let rst = new cv.Mat(video.height, video.width, cv.CV_8UC1);
      let begin = Date.now();
      let fx = 496.74244/2;
      let fy = 495.45232/2;
      let cx = 655.44564/2;
      let cy = 390.89756/2;
      let k1 = -0.222852;
      let k2 = 0.028990;
      let p1 = 0.000416;
      let p2 = -0.001841;
      cap.read(src);
      cv.cvtColor(src, dst, cv.COLOR_RGBA2GRAY);

      for(v = 0; v < video.height; v++){
        for(u = 0; u < video.width; u++){
          let x = (u - cx) / fx;
          let y = (v - cy) / fy;
          let r = Math.sqrt(x * x + y * y);
          let x_distorted = x * (1 + k1 * r * r + k2 * r * r * r * r) +
                            2 * p1 * x * y + p2 * (r * r + 2 * x * x);
          let y_distorted = y * (1 + k1 * r * r + k2 * r * r * r * r) +
                            2 * p2 * x * y + p1 * (r * r + 2 * y * y);
          let u_distorted = fx * x_distorted + cx;
          let v_distorted = fy * y_distorted + cy;
          if(u_distorted >= 0 && v_distorted >= 0 &&
             u_distorted < video.width && v_distorted < video.height){
              rst.ucharPtr(v, u)[0] = dst.ucharPtr(v_distorted, u_distorted)[0];
          }
          else{
            rst.ucharPtr(v, u)[0] = 0;
          }
        }
      }

      cv.imshow("canvasOutput", rst);
      src.delete();
      dst.delete();
      rst.delete();

      const videoPromise = document.getElementById('videoInput');
      const canvasPromise = document.getElementById('canvasOutput');
      // 'mobilenet_v1', 'mobilenet_v2', 'lite_mobilenet_v2'
      // Defaults to 'lite_mobilenet_v2' 
      // lite_mobilenet_v2 is fastest in inference speed.
      // mobilenet_v2 has the highest accuracy.
      const modelPromise = cocoSsd.load({ base: 'lite_mobilenet_v2' });
      Promise.all([modelPromise, videoPromise])
          .then(values => {
              detectFrame(values[1], values[0], "canvas0");
          })
          .catch(error => {
              console.error(error);
          });
      Promise.all([modelPromise, canvasPromise])
          .then(values => {
              detectFrame(values[1], values[0], "canvas1");
          })
          .catch(error => {
              console.error(error);
          });
        
      detectFrame = (video, model, canvasElement) => {
          // IMPORTANT: define video size before detection (equal to window size)
          // Usage: detect(Video element, max detections, threshold)
          model.detect(video).then(predictions => {
              // Debug output
              // console.log('Predictions: ', predictions);
              // Draw predictions
              renderPredictions(predictions, canvasElement);
          });
      };

      renderPredictions = (predictions, canvasElement) => {
          //console.log('Canvas_size: ', canvas.width, canvas.height);
          const canvas = document.getElementById(canvasElement);
          const ctx = document.getElementById(canvasElement).getContext("2d");
          ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
          // Font options
          const font = "14px sans-serif";
          ctx.font = font;
          ctx.textBaseline = "top";
          var personDetected = false;
          predictions.forEach(prediction => {
              const _class = prediction.class;
              // Draw person class only
              if (_class == "person") {
                  // Coordination conversion use real video size and window size
                  const x = prediction.bbox[0];
                  const y = prediction.bbox[1];
                  const width = prediction.bbox[2];
                  const height = prediction.bbox[3];
                  // Draw the bounding box.
                  ctx.strokeStyle = "#00FFFF";
                  ctx.lineWidth = 4;
                  ctx.strokeRect(x, y, width, height);
                  // Draw the label background.
                  ctx.fillStyle = "#00FFFF";
                  const textWidth = ctx.measureText(prediction.class).width;
                  const textHeight = parseInt(font, 10); // base 10
                  ctx.fillRect(x, y, textWidth + 4, textHeight + 4);
                  // Draw the text last to ensure it's on top.
                  ctx.fillStyle = "#000000";
                  ctx.fillText(prediction.class, x, y);
                  ctx.fillText(prediction.score.toFixed(2), x, y + height - textHeight);
                  personDetected = true;
              }
          });
      };

      // schedule next one.
      let delay = 1000/FPS - (Date.now() - begin);
      setTimeout(processVideo, delay);
    }
    function onOpenCvReady() {
      cv['onRuntimeInitialized']=()=>{
        console.log("OPENCVJS LOADED!");
        console.log(cv);
        //alert('OpenCV.js is ready.');
        // do all your work here
        // schedule first one.
        setTimeout(processVideo, 0);
      };
    }
    startVideo();
  </script>

  <!-- Load opencv.js. -->  
  <script src="https://docs.opencv.org/4.5.1/opencv.js" onload="onOpenCvReady();" type="text/javascript"></script>
  <!--script async src="opencv.js" onload="onOpenCvReady();" type="text/javascript"></script-->

  <!-- We will put our React component inside this div. -->
  <div id="root"></div>
  
</body>